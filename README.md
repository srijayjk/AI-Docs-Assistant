
# ğŸ§  AI Docs Assistant

An AI-powered document question-answering application built using FastAPI, Streamlit, ChromaDB, and LLMs (LLaMA3 via Ollama). Upload PDFs/images and ask natural language questions about the content.

---

## ğŸš€ Features

- ğŸ“„ Upload documents (PDFs/images)
- ğŸ¤– Ask questions in natural language
- ğŸ§  Uses RAG (Retrieval-Augmented Generation)
- ğŸ’¾ Vector store with ChromaDB
- ğŸ§° GPU-enabled embeddings via SentenceTransformer
- ğŸ” API key-protected endpoints
- ğŸ“Š Streamlit UI with file list, history, and confidence scores
- ğŸ“¦ Dockerized backend for easy deployment

---
     [PDF/Image]             <-- User upload
          |
        /upload              <-- FastAPI
          |
        OCR/Text             <-- Extract content from file
          |
    Chunk & Embed            <-- Text is split into chunks
    via Sentence-    
    Transformers     
          |
     Store in ChromaDB       <-- Persistent vector DB
          |
        ask                  <-- User query
          |
     Query ChromaDB          <-- Retrieve top relevant chunks
          |
      Prompt LLM             <-- With context + question
          |
     Return Answer           <-- Via FastAPI
---

## ğŸ“‚ Project Structure
```bash
AI-Docs-Assistant/
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ main.py # FastAPI app
â”‚ â”œâ”€â”€ auth.py # API key verification
â”‚ â”œâ”€â”€ ingest.py # PDF/image text extraction
â”‚ â”œâ”€â”€ rag.py # RAG engine & Chroma vector store
â”œâ”€â”€ ui/
â”‚ â””â”€â”€ streamlit_app.py # Frontend interface
â”œâ”€â”€ data/docs/ # Uploaded documents
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .env
â””â”€â”€ README.md
```
---

## âš™ï¸ Prerequisites

- Python 3.10+
- Docker
- Ollama (for running local LLMs like LLaMA3)
  ```bash
  curl -fsSL https://ollama.com/install.sh | sh
  ollama run llama3
  ```

ğŸ§ª Setup Locally (Without Docker)
1. Clone the repo
```bash
git clone https://github.com/yourusername/AI-Docs-Assistant.git
cd AI-Docs-Assistant
```
2. Create and activate virtual environment
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```
3. Install dependencies
```bash
pip install -r requirements.txt
```
4. Run FastAPI backend
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```
5. Run Streamlit frontend
```bash
streamlit run ui/streamlit_app.py
```


## ğŸ” API Authentication
Add your key in .env:

API_KEY=srijaykey123
-H "X-API-Key: srijaykey123"


## ğŸ“¦ Docker Deployment
1. Build the Docker image
```bash
docker compose up --build
```
2. Run the container
```bash
docker-compose up
```
(Optional) Use Docker Compose (if docker-compose.yml is added)

ğŸ§  Example Usage
3. Upload a Document
```bash
curl -X POST http://localhost:8000/upload \
  -H "X-API-Key: srijaykey123" \
  -F "file=@data/docs/sample.pdf"
```
4. Ask a Question
```bash
curl -X POST http://localhost:8000/ask \
  -H "X-API-Key: srijaykey123" \
  -H "Content-Type: application/json" \
  -d '{"question": "What is the document about?"}'
```

## ğŸ›  Tech Stack
- Backend: FastAPI
- Frontend: Streamlit
- Vector DB: Chroma
- Embeddings: RecursiveCharacterTextSplitter
- OCR/Parsing: unstructured, PyMuPDF, Tesseract
- LLM: LLaMA3 via Ollama


## ğŸ§¹ Cleanup / Reset
You can clear the vector store manually:
```bash
rm -rf chroma_store/
```
Or programmatically in your app.


## ğŸ“¬ Contact
Made with â¤ï¸ by Srijay Kolvekar

ğŸ“„ License
MIT License

---
